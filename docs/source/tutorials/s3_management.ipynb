{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/sci-ndp/ndp-ep-py/blob/main/docs/source/tutorials/s3_management.ipynb)\n",
    "[![Binder](https://mybinder.org/badge_logo.svg)](https://mybinder.org/v2/gh/sci-ndp/ndp-ep-py/main?filepath=docs%2Fsource%2Ftutorials%2Fs3_management.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# S3 Buckets and Objects Management Tutorial\n",
    "\n",
    "Welcome to the comprehensive tutorial on S3 buckets and objects management with the NDP EP Python client!\n",
    "\n",
    "## 🎯 What You'll Learn\n",
    "\n",
    "This tutorial covers complete S3 data management workflows:\n",
    "\n",
    "- **🪣 Bucket Management**: Create, list, inspect, and delete S3 buckets\n",
    "- **📁 Object Operations**: Upload, download, list, and delete objects\n",
    "- **📊 Metadata Handling**: Work with object metadata and properties\n",
    "- **🔗 Presigned URLs**: Generate secure temporary URLs for uploads/downloads\n",
    "- **📈 Batch Operations**: Efficiently manage multiple files\n",
    "- **🛡️ Error Handling**: Robust error handling and best practices\n",
    "- **🧹 Resource Cleanup**: Proper cleanup of S3 resources\n",
    "\n",
    "## 🔧 Use Cases\n",
    "\n",
    "Perfect for:\n",
    "- **Data Lake Management**: Managing large datasets in S3\n",
    "- **File Storage**: Storing and retrieving files programmatically\n",
    "- **Data Sharing**: Creating secure links for data sharing\n",
    "- **Backup Systems**: Automated backup and restore workflows\n",
    "- **Data Processing**: ETL pipelines with S3 integration\n",
    "\n",
    "## ⚠️ Prerequisites\n",
    "\n",
    "- Valid NDP EP API credentials (token)\n",
    "- S3 permissions on the target API instance\n",
    "- Basic understanding of S3 concepts (buckets, objects, keys)\n",
    "\n",
    "## 🛡️ Safety Note\n",
    "\n",
    "This tutorial creates and deletes S3 resources. Always test on development systems first and ensure proper cleanup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required libraries\n",
    "!pip install ndp-ep\n",
    "\n",
    "# Import required modules\n",
    "import io\n",
    "import json\n",
    "import time\n",
    "import getpass\n",
    "from typing import List, Dict, Any, Optional\n",
    "from ndp_ep import APIClient\n",
    "\n",
    "print(\"✅ Libraries installed and imported successfully!\")\n",
    "print(\"📚 Ready to start S3 management tutorial\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 🔐 Authentication and Client Setup\n",
    "\n",
    "First, let's configure the client with your credentials for S3 operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Interactive API configuration\nprint(\"🔧 S3 Management Configuration\")\nprint(\"=\" * 35)\n\n# Get API base URL\napi_url = input(\"Enter API base URL [http://localhost:8000]: \").strip()\nif not api_url:\n    api_url = \"http://localhost:8000\"\n\nprint(f\"📡 API URL: {api_url}\")\n\n# Get API token securely\nprint(\"\\n🔐 Authentication\")\nprint(\"Please enter your API token (it will be hidden):\")\napi_token = getpass.getpass(\"API Token: \")\n\nif not api_token.strip():\n    raise ValueError(\"❌ API token is required for S3 operations\")\n\nprint(\"✅ Credentials configured securely\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Initialize and test the API client\nprint(\"🚀 Initializing S3-enabled API Client...\")\n\ntry:\n    client = APIClient(base_url=api_url, token=api_token)\n    \n    # Test basic connection\n    try:\n        system_status = client.get_system_status()\n        print(\"✅ API client initialized successfully\")\n        print(f\"🌐 Connected to: {api_url}\")\n        print(\"🔑 Authentication verified\")\n        \n        # Test S3 functionality by listing buckets\n        try:\n            buckets = client.list_buckets()\n            print(f\"🪣 S3 functionality confirmed - {len(buckets)} buckets found\")\n        except Exception as e:\n            print(f\"⚠️  S3 functionality test: {e}\")\n            print(\"💡 S3 features may require additional setup\")\n            \n    except Exception as e:\n        print(f\"⚠️  API connection test failed: {e}\")\n        print(\"💡 Continuing in demo mode - some features may not work\")\n        print(\"✅ Client object created successfully\")\n    \nexcept Exception as e:\n    print(f\"❌ Failed to initialize client: {e}\")\n    print(\"💡 Please check your credentials and API URL\")\n    raise"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 📋 Helper Functions and Configuration\n",
    "\n",
    "Let's create utility functions for our S3 operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration for the tutorial\n",
    "TUTORIAL_PREFIX = \"tutorial\"\n",
    "TUTORIAL_BUCKET = f\"{TUTORIAL_PREFIX}-bucket-{int(time.time())}\"\n",
    "SAMPLE_FILES = {\n",
    "    \"data/sample.txt\": \"This is a sample text file for S3 tutorial.\",\n",
    "    \"data/config.json\": '{\"environment\": \"tutorial\", \"version\": \"1.0\"}',\n",
    "    \"logs/app.log\": \"2024-01-01 10:00:00 INFO Application started\\n2024-01-01 10:01:00 INFO Processing data\",\n",
    "    \"images/placeholder.txt\": \"This represents an image file in the tutorial\"\n",
    "}\n",
    "\n",
    "print(\"📊 Tutorial Configuration\")\n",
    "print(\"=\" * 30)\n",
    "print(f\"Bucket name: {TUTORIAL_BUCKET}\")\n",
    "print(f\"Sample files: {len(SAMPLE_FILES)} files\")\n",
    "print(f\"Prefix: {TUTORIAL_PREFIX}\")\n",
    "\n",
    "# Storage for tracking created resources\n",
    "created_buckets = []\n",
    "uploaded_objects = []\n",
    "operation_log = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_operation(operation: str, resource_type: str, \n",
    "                  resource_name: str, success: bool, \n",
    "                  details: str = \"\") -> None:\n",
    "    \"\"\"\n",
    "    Log an S3 operation for tracking and debugging.\n",
    "    \n",
    "    Args:\n",
    "        operation: Type of operation (create, upload, download, delete)\n",
    "        resource_type: Type of resource (bucket, object)\n",
    "        resource_name: Name of the resource\n",
    "        success: Whether the operation was successful\n",
    "        details: Additional details or error messages\n",
    "    \"\"\"\n",
    "    timestamp = time.strftime(\"%H:%M:%S\")\n",
    "    status = \"✅\" if success else \"❌\"\n",
    "    \n",
    "    log_entry = {\n",
    "        \"timestamp\": timestamp,\n",
    "        \"operation\": operation,\n",
    "        \"resource_type\": resource_type,\n",
    "        \"resource_name\": resource_name,\n",
    "        \"success\": success,\n",
    "        \"details\": details\n",
    "    }\n",
    "    \n",
    "    operation_log.append(log_entry)\n",
    "    print(f\"{status} [{timestamp}] {operation.title()} {resource_type}: {resource_name}\")\n",
    "    \n",
    "    if details and not success:\n",
    "        print(f\"   └─ Error: {details}\")\n",
    "    elif details and success:\n",
    "        print(f\"   └─ {details}\")\n",
    "\n",
    "\n",
    "def format_file_size(size_bytes: int) -> str:\n",
    "    \"\"\"\n",
    "    Format file size in human-readable format.\n",
    "    \n",
    "    Args:\n",
    "        size_bytes: Size in bytes\n",
    "        \n",
    "    Returns:\n",
    "        Formatted size string\n",
    "    \"\"\"\n",
    "    for unit in ['B', 'KB', 'MB', 'GB']:\n",
    "        if size_bytes < 1024.0:\n",
    "            return f\"{size_bytes:.1f} {unit}\"\n",
    "        size_bytes /= 1024.0\n",
    "    return f\"{size_bytes:.1f} TB\"\n",
    "\n",
    "\n",
    "def safe_s3_operation(func, *args, operation_name: str, **kwargs):\n",
    "    \"\"\"\n",
    "    Safely execute an S3 operation with error handling.\n",
    "    \n",
    "    Args:\n",
    "        func: Function to execute\n",
    "        operation_name: Description of the operation\n",
    "        *args, **kwargs: Arguments for the function\n",
    "        \n",
    "    Returns:\n",
    "        Result of the function or None if failed\n",
    "    \"\"\"\n",
    "    try:\n",
    "        result = func(*args, **kwargs)\n",
    "        return result\n",
    "    except ValueError as e:\n",
    "        print(f\"❌ {operation_name} failed: {e}\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"⚠️  Unexpected error in {operation_name}: {e}\")\n",
    "        return None\n",
    "\n",
    "print(\"🔧 Helper functions defined successfully\")\n",
    "print(\"📝 Ready for S3 operations\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 🪣 Bucket Management\n",
    "\n",
    "Let's start with basic bucket operations: listing, creating, and inspecting buckets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List existing buckets\n",
    "print(\"🪣 Current S3 Buckets\")\n",
    "print(\"=\" * 25)\n",
    "\n",
    "try:\n",
    "    existing_buckets = client.list_buckets()\n",
    "    print(f\"📋 Found {len(existing_buckets)} existing buckets:\")\n",
    "    \n",
    "    for i, bucket in enumerate(existing_buckets, 1):\n",
    "        bucket_name = bucket.get('name', bucket.get('Name', 'Unknown'))\n",
    "        created_date = bucket.get('created', bucket.get('CreationDate', 'Unknown'))\n",
    "        print(f\"   {i}. {bucket_name} (Created: {created_date})\")\n",
    "    \n",
    "    if not existing_buckets:\n",
    "        print(\"   📭 No buckets found\")\n",
    "        \n",
    "    log_operation(\"list\", \"buckets\", \"all\", True, f\"Found {len(existing_buckets)} buckets\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"❌ Failed to list buckets: {e}\")\n",
    "    log_operation(\"list\", \"buckets\", \"all\", False, str(e))\n",
    "    existing_buckets = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new bucket for our tutorial\n",
    "print(f\"\\n🔨 Creating Tutorial Bucket: {TUTORIAL_BUCKET}\")\n",
    "print(\"=\" * 45)\n",
    "\n",
    "try:\n",
    "    # Check if bucket already exists\n",
    "    bucket_exists = any(\n",
    "        bucket.get('name', bucket.get('Name', '')) == TUTORIAL_BUCKET \n",
    "        for bucket in existing_buckets\n",
    "    )\n",
    "    \n",
    "    if bucket_exists:\n",
    "        print(f\"ℹ️  Bucket '{TUTORIAL_BUCKET}' already exists\")\n",
    "        log_operation(\"check\", \"bucket\", TUTORIAL_BUCKET, True, \"Already exists\")\n",
    "    else:\n",
    "        # Create the bucket (use name parameter for API compatibility)\n",
    "        result = client.create_bucket(TUTORIAL_BUCKET, name=TUTORIAL_BUCKET)\n",
    "        print(f\"✅ Bucket created successfully\")\n",
    "        print(f\"📍 Bucket name: {TUTORIAL_BUCKET}\")\n",
    "        \n",
    "        if 'location' in result:\n",
    "            print(f\"🌍 Location: {result['location']}\")\n",
    "        \n",
    "        created_buckets.append(TUTORIAL_BUCKET)\n",
    "        log_operation(\"create\", \"bucket\", TUTORIAL_BUCKET, True, \"Successfully created\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"❌ Failed to create bucket: {e}\")\n",
    "    log_operation(\"create\", \"bucket\", TUTORIAL_BUCKET, False, str(e))\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get detailed information about our bucket\n",
    "print(f\"\\n🔍 Inspecting Bucket: {TUTORIAL_BUCKET}\")\n",
    "print(\"=\" * 35)\n",
    "\n",
    "try:\n",
    "    bucket_info = client.get_bucket_info(TUTORIAL_BUCKET)\n",
    "    print(\"✅ Bucket information retrieved:\")\n",
    "    \n",
    "    # Display bucket details\n",
    "    for key, value in bucket_info.items():\n",
    "        if key.lower() in ['name', 'created', 'location', 'region', 'versioning']:\n",
    "            print(f\"   📋 {key.title()}: {value}\")\n",
    "    \n",
    "    log_operation(\"inspect\", \"bucket\", TUTORIAL_BUCKET, True, \"Information retrieved\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"❌ Failed to get bucket info: {e}\")\n",
    "    log_operation(\"inspect\", \"bucket\", TUTORIAL_BUCKET, False, str(e))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. 📁 Object Upload Operations\n",
    "\n",
    "Now let's upload various types of files to demonstrate object management."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload sample files to the bucket\n",
    "print(f\"📤 Uploading Sample Files to {TUTORIAL_BUCKET}\")\n",
    "print(\"=\" * 45)\n",
    "\n",
    "upload_start_time = time.time()\n",
    "successful_uploads = 0\n",
    "failed_uploads = 0\n",
    "\n",
    "for object_key, content in SAMPLE_FILES.items():\n",
    "    try:\n",
    "        # Convert string content to bytes\n",
    "        file_data = content.encode('utf-8')\n",
    "        \n",
    "        # Determine content type based on file extension\n",
    "        if object_key.endswith('.json'):\n",
    "            content_type = 'application/json'\n",
    "        elif object_key.endswith('.txt') or object_key.endswith('.log'):\n",
    "            content_type = 'text/plain'\n",
    "        else:\n",
    "            content_type = 'application/octet-stream'\n",
    "        \n",
    "        # Upload the object\n",
    "        result = client.upload_object(\n",
    "            bucket_name=TUTORIAL_BUCKET,\n",
    "            object_key=object_key,\n",
    "            file_data=file_data,\n",
    "            content_type=content_type\n",
    "        )\n",
    "        \n",
    "        file_size = len(file_data)\n",
    "        print(f\"✅ Uploaded: {object_key} ({format_file_size(file_size)})\")\n",
    "        \n",
    "        uploaded_objects.append({\n",
    "            'bucket': TUTORIAL_BUCKET,\n",
    "            'key': object_key,\n",
    "            'size': file_size,\n",
    "            'content_type': content_type\n",
    "        })\n",
    "        \n",
    "        successful_uploads += 1\n",
    "        log_operation(\"upload\", \"object\", object_key, True, \n",
    "                     f\"Size: {format_file_size(file_size)}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        failed_uploads += 1\n",
    "        print(f\"❌ Failed to upload {object_key}: {e}\")\n",
    "        log_operation(\"upload\", \"object\", object_key, False, str(e))\n",
    "\n",
    "upload_duration = time.time() - upload_start_time\n",
    "\n",
    "print(f\"\\n📊 Upload Summary:\")\n",
    "print(f\"   ✅ Successful: {successful_uploads}/{len(SAMPLE_FILES)}\")\n",
    "print(f\"   ❌ Failed: {failed_uploads}/{len(SAMPLE_FILES)}\")\n",
    "print(f\"   ⏱️  Duration: {upload_duration:.2f} seconds\")\n",
    "\n",
    "if successful_uploads > 0:\n",
    "    total_size = sum(obj['size'] for obj in uploaded_objects)\n",
    "    print(f\"   📦 Total uploaded: {format_file_size(total_size)}\")\n",
    "    print(f\"   🚀 Average speed: {successful_uploads/upload_duration:.1f} files/second\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. 📋 Object Listing and Discovery\n",
    "\n",
    "Let's explore different ways to list and discover objects in our bucket."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List all objects in the bucket\n",
    "print(f\"📋 Listing All Objects in {TUTORIAL_BUCKET}\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "try:\n",
    "    all_objects = client.list_objects(TUTORIAL_BUCKET)\n",
    "    print(f\"📁 Found {len(all_objects)} objects:\")\n",
    "    \n",
    "    for i, obj in enumerate(all_objects, 1):\n",
    "        obj_key = obj.get('key', obj.get('Key', 'Unknown'))\n",
    "        obj_size = obj.get('size', obj.get('Size', 0))\n",
    "        obj_modified = obj.get('last_modified', obj.get('LastModified', 'Unknown'))\n",
    "        \n",
    "        print(f\"   {i}. {obj_key}\")\n",
    "        print(f\"      📦 Size: {format_file_size(obj_size)}\")\n",
    "        print(f\"      📅 Modified: {obj_modified}\")\n",
    "    \n",
    "    log_operation(\"list\", \"objects\", \"all\", True, f\"Found {len(all_objects)} objects\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"❌ Failed to list objects: {e}\")\n",
    "    log_operation(\"list\", \"objects\", \"all\", False, str(e))\n",
    "    all_objects = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List objects with prefix filtering\n",
    "print(f\"\\n🔍 Filtering Objects by Prefix\")\n",
    "print(\"=\" * 35)\n",
    "\n",
    "prefixes_to_test = [\"data/\", \"logs/\", \"images/\"]\n",
    "\n",
    "for prefix in prefixes_to_test:\n",
    "    try:\n",
    "        filtered_objects = client.list_objects(TUTORIAL_BUCKET, prefix=prefix)\n",
    "        print(f\"\\n📂 Objects with prefix '{prefix}': {len(filtered_objects)} found\")\n",
    "        \n",
    "        for obj in filtered_objects:\n",
    "            obj_key = obj.get('key', obj.get('Key', 'Unknown'))\n",
    "            obj_size = obj.get('size', obj.get('Size', 0))\n",
    "            print(f\"   📄 {obj_key} ({format_file_size(obj_size)})\")\n",
    "        \n",
    "        log_operation(\"filter\", \"objects\", prefix, True, \n",
    "                     f\"Found {len(filtered_objects)} objects\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Failed to filter objects with prefix '{prefix}': {e}\")\n",
    "        log_operation(\"filter\", \"objects\", prefix, False, str(e))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. 📥 Object Download and Metadata\n",
    "\n",
    "Let's download objects and examine their metadata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download and examine a sample object\n",
    "sample_object_key = \"data/sample.txt\"\n",
    "\n",
    "print(f\"📥 Downloading Object: {sample_object_key}\")\n",
    "print(\"=\" * 35)\n",
    "\n",
    "try:\n",
    "    # Download the object\n",
    "    object_data = client.download_object(TUTORIAL_BUCKET, sample_object_key)\n",
    "    \n",
    "    # Decode and display content\n",
    "    content = object_data.decode('utf-8')\n",
    "    print(f\"✅ Downloaded successfully\")\n",
    "    print(f\"📦 Size: {format_file_size(len(object_data))}\")\n",
    "    print(f\"📄 Content preview:\")\n",
    "    print(f\"   {content[:100]}{'...' if len(content) > 100 else ''}\")\n",
    "    \n",
    "    log_operation(\"download\", \"object\", sample_object_key, True, \n",
    "                 f\"Size: {format_file_size(len(object_data))}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"❌ Failed to download object: {e}\")\n",
    "    log_operation(\"download\", \"object\", sample_object_key, False, str(e))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get object metadata\n",
    "print(f\"\\n🔍 Object Metadata: {sample_object_key}\")\n",
    "print(\"=\" * 30)\n",
    "\n",
    "try:\n",
    "    metadata = client.get_object_metadata(TUTORIAL_BUCKET, sample_object_key)\n",
    "    print(\"✅ Metadata retrieved:\")\n",
    "    \n",
    "    # Display relevant metadata fields\n",
    "    metadata_fields = [\n",
    "        'content_type', 'content_length', 'last_modified', \n",
    "        'etag', 'content_encoding', 'cache_control'\n",
    "    ]\n",
    "    \n",
    "    for field in metadata_fields:\n",
    "        if field in metadata:\n",
    "            value = metadata[field]\n",
    "            if field == 'content_length':\n",
    "                value = f\"{value} bytes ({format_file_size(int(value))})\"\n",
    "            print(f\"   📋 {field.replace('_', ' ').title()}: {value}\")\n",
    "    \n",
    "    # Display custom metadata if any\n",
    "    custom_metadata = {k: v for k, v in metadata.items() \n",
    "                      if k.startswith('x-amz-meta-') or k not in metadata_fields}\n",
    "    \n",
    "    if custom_metadata:\n",
    "        print(\"   🏷️  Custom Metadata:\")\n",
    "        for key, value in custom_metadata.items():\n",
    "            print(f\"      {key}: {value}\")\n",
    "    \n",
    "    log_operation(\"metadata\", \"object\", sample_object_key, True, \"Retrieved successfully\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"❌ Failed to get metadata: {e}\")\n",
    "    log_operation(\"metadata\", \"object\", sample_object_key, False, str(e))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. 🔗 Presigned URLs for Secure Sharing\n",
    "\n",
    "Learn how to generate secure temporary URLs for sharing files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate presigned download URL\n",
    "print(f\"🔗 Generating Presigned URLs\")\n",
    "print(\"=\" * 30)\n",
    "\n",
    "target_object = \"data/config.json\"\n",
    "expiration_time = 3600  # 1 hour\n",
    "\n",
    "try:\n",
    "    # Generate download URL\n",
    "    download_url_data = client.generate_presigned_download_url(\n",
    "        bucket_name=TUTORIAL_BUCKET,\n",
    "        object_key=target_object,\n",
    "        expiration=expiration_time\n",
    "    )\n",
    "    \n",
    "    print(f\"✅ Download URL generated for: {target_object}\")\n",
    "    print(f\"⏰ Expires in: {expiration_time} seconds ({expiration_time//3600}h {(expiration_time%3600)//60}m)\")\n",
    "    \n",
    "    if 'url' in download_url_data:\n",
    "        url = download_url_data['url']\n",
    "        print(f\"🔗 URL: {url[:50]}...{url[-20:] if len(url) > 70 else url[50:]}\")\n",
    "    \n",
    "    log_operation(\"presign_download\", \"object\", target_object, True, \n",
    "                 f\"Expires in {expiration_time}s\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"❌ Failed to generate download URL: {e}\")\n",
    "    log_operation(\"presign_download\", \"object\", target_object, False, str(e))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate presigned upload URL\n",
    "print(f\"\\n📤 Generating Presigned Upload URL\")\n",
    "print(\"=\" * 35)\n",
    "\n",
    "upload_object_key = \"uploads/new-file.txt\"\n",
    "upload_expiration = 1800  # 30 minutes\n",
    "\n",
    "try:\n",
    "    # Generate upload URL\n",
    "    upload_url_data = client.generate_presigned_upload_url(\n",
    "        bucket_name=TUTORIAL_BUCKET,\n",
    "        object_key=upload_object_key,\n",
    "        expiration=upload_expiration\n",
    "    )\n",
    "    \n",
    "    print(f\"✅ Upload URL generated for: {upload_object_key}\")\n",
    "    print(f\"⏰ Expires in: {upload_expiration} seconds ({upload_expiration//60}m)\")\n",
    "    \n",
    "    if 'url' in upload_url_data:\n",
    "        url = upload_url_data['url']\n",
    "        print(f\"🔗 URL: {url[:50]}...{url[-20:] if len(url) > 70 else url[50:]}\")\n",
    "    \n",
    "    if 'fields' in upload_url_data:\n",
    "        print(f\"📋 Form fields: {len(upload_url_data['fields'])} required\")\n",
    "    \n",
    "    log_operation(\"presign_upload\", \"object\", upload_object_key, True, \n",
    "                 f\"Expires in {upload_expiration}s\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"❌ Failed to generate upload URL: {e}\")\n",
    "    log_operation(\"presign_upload\", \"object\", upload_object_key, False, str(e))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. 📊 Batch Operations and Advanced Management\n",
    "\n",
    "Demonstrate efficient batch operations for managing multiple objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create additional files for batch operations\n",
    "print(\"📦 Creating Additional Files for Batch Demo\")\n",
    "print(\"=\" * 45)\n",
    "\n",
    "batch_files = {}\n",
    "for i in range(1, 6):\n",
    "    file_key = f\"batch/file_{i:02d}.txt\"\n",
    "    file_content = f\"This is batch file number {i}\\nGenerated for tutorial purposes\\nTimestamp: {time.strftime('%Y-%m-%d %H:%M:%S')}\"\n",
    "    batch_files[file_key] = file_content\n",
    "\n",
    "batch_start_time = time.time()\n",
    "batch_successful = 0\n",
    "batch_failed = 0\n",
    "\n",
    "for object_key, content in batch_files.items():\n",
    "    try:\n",
    "        file_data = content.encode('utf-8')\n",
    "        \n",
    "        result = client.upload_object(\n",
    "            bucket_name=TUTORIAL_BUCKET,\n",
    "            object_key=object_key,\n",
    "            file_data=file_data,\n",
    "            content_type='text/plain'\n",
    "        )\n",
    "        \n",
    "        batch_successful += 1\n",
    "        uploaded_objects.append({\n",
    "            'bucket': TUTORIAL_BUCKET,\n",
    "            'key': object_key,\n",
    "            'size': len(file_data),\n",
    "            'content_type': 'text/plain'\n",
    "        })\n",
    "        \n",
    "        print(f\"✅ Batch upload: {object_key}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        batch_failed += 1\n",
    "        print(f\"❌ Batch upload failed: {object_key} - {e}\")\n",
    "\n",
    "batch_duration = time.time() - batch_start_time\n",
    "\n",
    "print(f\"\\n📈 Batch Upload Results:\")\n",
    "print(f\"   ✅ Successful: {batch_successful}/{len(batch_files)}\")\n",
    "print(f\"   ❌ Failed: {batch_failed}/{len(batch_files)}\")\n",
    "print(f\"   ⏱️  Duration: {batch_duration:.2f} seconds\")\n",
    "print(f\"   🚀 Rate: {batch_successful/batch_duration:.1f} files/second\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. 🧹 Cleanup and Resource Management\n",
    "\n",
    "Let's properly clean up all the resources we created during this tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confirmation before cleanup\n",
    "print(\"🧹 CLEANUP CONFIRMATION\")\n",
    "print(\"=\" * 30)\n",
    "print(f\"About to delete:\")\n",
    "print(f\"   📁 {len(uploaded_objects)} objects\")\n",
    "print(f\"   🪣 1 bucket ({TUTORIAL_BUCKET})\")\n",
    "print(f\"\\nTotal resources: {len(uploaded_objects) + 1}\")\n",
    "\n",
    "# Get user confirmation\n",
    "confirmation = input(\"\\nProceed with cleanup? (yes/no): \").strip().lower()\n",
    "\n",
    "if confirmation != 'yes':\n",
    "    print(\"🚫 Cleanup cancelled by user\")\n",
    "    print(\"💡 Resources remain in S3 for manual cleanup\")\n",
    "else:\n",
    "    print(\"\\n✅ Proceeding with cleanup...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete all objects (only if user confirmed)\n",
    "if confirmation == 'yes':\n",
    "    print(f\"\\n🗑️  Deleting Objects from {TUTORIAL_BUCKET}\")\n",
    "    print(\"=\" * 40)\n",
    "    \n",
    "    cleanup_start_time = time.time()\n",
    "    deleted_objects = 0\n",
    "    failed_deletions = 0\n",
    "    \n",
    "    # Get current objects (in case some were added/removed)\n",
    "    try:\n",
    "        current_objects = client.list_objects(TUTORIAL_BUCKET)\n",
    "        objects_to_delete = [obj.get('key', obj.get('Key', '')) for obj in current_objects]\n",
    "        \n",
    "        print(f\"📋 Found {len(objects_to_delete)} objects to delete\")\n",
    "        \n",
    "        for i, object_key in enumerate(objects_to_delete, 1):\n",
    "            try:\n",
    "                client.delete_object(TUTORIAL_BUCKET, object_key)\n",
    "                deleted_objects += 1\n",
    "                print(f\"✅ Deleted: {object_key}\")\n",
    "                log_operation(\"delete\", \"object\", object_key, True, \"Cleanup\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                failed_deletions += 1\n",
    "                print(f\"❌ Failed to delete {object_key}: {e}\")\n",
    "                log_operation(\"delete\", \"object\", object_key, False, str(e))\n",
    "            \n",
    "            # Small delay to avoid overwhelming the API\n",
    "            if i % 5 == 0:\n",
    "                time.sleep(0.1)\n",
    "        \n",
    "        cleanup_duration = time.time() - cleanup_start_time\n",
    "        \n",
    "        print(f\"\\n📈 Object Deletion Summary:\")\n",
    "        print(f\"   ✅ Deleted: {deleted_objects}/{len(objects_to_delete)}\")\n",
    "        print(f\"   ❌ Failed: {failed_deletions}/{len(objects_to_delete)}\")\n",
    "        print(f\"   ⏱️  Duration: {cleanup_duration:.2f} seconds\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Failed to list objects for deletion: {e}\")\n",
    "else:\n",
    "    print(\"\\n⏭️  Skipping object deletion (cleanup cancelled)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete the bucket (only if user confirmed and objects were deleted)\n",
    "if confirmation == 'yes':\n",
    "    print(f\"\\n🪣 Deleting Bucket: {TUTORIAL_BUCKET}\")\n",
    "    print(\"=\" * 30)\n",
    "    \n",
    "    try:\n",
    "        # Verify bucket is empty\n",
    "        remaining_objects = client.list_objects(TUTORIAL_BUCKET)\n",
    "        \n",
    "        if len(remaining_objects) == 0:\n",
    "            # Safe to delete bucket\n",
    "            result = client.delete_bucket(TUTORIAL_BUCKET)\n",
    "            print(f\"✅ Bucket '{TUTORIAL_BUCKET}' deleted successfully\")\n",
    "            log_operation(\"delete\", \"bucket\", TUTORIAL_BUCKET, True, \"Cleanup complete\")\n",
    "        else:\n",
    "            print(f\"⚠️  Bucket '{TUTORIAL_BUCKET}' still contains {len(remaining_objects)} objects\")\n",
    "            print(f\"   Bucket will not be deleted for safety\")\n",
    "            log_operation(\"delete\", \"bucket\", TUTORIAL_BUCKET, False, \n",
    "                         f\"Still contains {len(remaining_objects)} objects\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Failed to delete bucket '{TUTORIAL_BUCKET}': {e}\")\n",
    "        log_operation(\"delete\", \"bucket\", TUTORIAL_BUCKET, False, str(e))\n",
    "else:\n",
    "    print(\"\\n⏭️  Skipping bucket deletion (cleanup cancelled)\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}