{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/sci-ndp/ndp-ep-py/blob/main/docs/source/tutorials/s3_to_dataset_integration.ipynb)\n",
    "[![Binder](https://mybinder.org/badge_logo.svg)](https://mybinder.org/v2/gh/sci-ndp/ndp-ep-py/main?filepath=docs%2Fsource%2Ftutorials%2Fs3_to_dataset_integration.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# S3 Object to Dataset Integration Tutorial\n",
    "\n",
    "Welcome to the comprehensive tutorial on integrating S3 objects with dataset registration!\n",
    "\n",
    "## 🎯 What You'll Learn\n",
    "\n",
    "This tutorial demonstrates a complete data publishing workflow:\n",
    "\n",
    "- **📤 S3 Object Upload**: Upload data files to S3 buckets\n",
    "- **🔗 URL Generation**: Generate secure access URLs for S3 objects\n",
    "- **📊 Metadata Extraction**: Extract and enrich object metadata\n",
    "- **📋 Dataset Registration**: Register datasets with S3 URLs and metadata\n",
    "- **🔍 Resource Discovery**: Search and verify registered datasets\n",
    "- **🧹 Resource Management**: Proper cleanup and lifecycle management\n",
    "\n",
    "## 🔧 Use Cases\n",
    "\n",
    "Perfect for:\n",
    "- **Data Publishing**: Publishing research datasets with metadata\n",
    "- **Data Lake Integration**: Connecting S3 storage with catalog systems\n",
    "- **Automated Workflows**: ETL pipelines with dataset registration\n",
    "- **Data Sharing**: Creating discoverable data resources\n",
    "- **Research Data Management**: Academic data publication workflows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required libraries\\n\n",
    "!pip install ndp-ep\\n\\n\n",
    "# Import required modules\\n\n",
    "import io\\n\n",
    "import json\\n\n",
    "import time\\n\n",
    "import getpass\\n\n",
    "import hashlib\\n\n",
    "from datetime import datetime, timezone\\n\n",
    "from typing import List, Dict, Any, Optional\\n\n",
    "from ndp_ep import APIClient\\n\\n\n",
    "print(\\\"✅ Libraries installed and imported successfully!\\\")\\n\n",
    "print(\\\"📚 Ready to start S3 to Dataset integration tutorial\\\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 🔐 Setup and Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\\n\n",
    "api_url = input(\\\"Enter API base URL [http://localhost:8000]: \\\").strip()\\n\n",
    "if not api_url:\\n\n",
    "    api_url = \\\"http://localhost:8000\\\"\\n\\n\n",
    "api_token = getpass.getpass(\\\"API Token: \\\")\\n\\n\n",
    "if not api_token.strip():\\n\n",
    "    raise ValueError(\\\"❌ API token is required\\\")\\n\\n\n",
    "print(\\\"✅ Configuration complete\\\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize client\\n\n",
    "client = APIClient(base_url=api_url, token=api_token)\\n\\n\n",
    "# Test functionality\\n\n",
    "system_status = client.get_system_status()\\n\n",
    "buckets = client.list_buckets()\\n\n",
    "orgs = client.list_organizations(server=\\\"local\\\")\\n\\n\n",
    "print(\\\"✅ Client initialized successfully\\\")\\n\n",
    "print(f\\\"🌐 Connected to: {api_url}\\\")\\n\n",
    "print(f\\\"🪣 S3 buckets available: {len(buckets)}\\\")\\n\n",
    "print(f\\\"🏢 Organizations available: {len(orgs)}\\\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 📤 Step 1: Upload File to S3 with Metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Workflow configuration\\n\n",
    "WORKFLOW_PREFIX = \\\"s3integration\\\"\\n\n",
    "TIMESTAMP = int(time.time())\\n\n",
    "WORKFLOW_BUCKET = f\\\"{WORKFLOW_PREFIX}{TIMESTAMP}\\\"\\n\n",
    "DATASET_ORG = orgs[0] if orgs else \\\"example_org_name\\\"\\n\\n\n",
    "print(f\\\"📊 Workflow Configuration:\\\")\\n\n",
    "print(f\\\"   Bucket: {WORKFLOW_BUCKET}\\\")\\n\n",
    "print(f\\\"   Organization: {DATASET_ORG}\\\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create S3 bucket\\n\n",
    "print(f\\\"🪣 Creating S3 bucket: {WORKFLOW_BUCKET}\\\")\\n\\n\n",
    "try:\\n\n",
    "    bucket_result = client.create_bucket(WORKFLOW_BUCKET, name=WORKFLOW_BUCKET)\\n\n",
    "    print(f\\\"✅ Bucket created successfully\\\")\\n\n",
    "    print(f\\\"📍 Bucket name: {WORKFLOW_BUCKET}\\\")\\n\n",
    "except Exception as e:\\n\n",
    "    print(f\\\"❌ Bucket creation failed: {e}\\\")\\n\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample research data\n",
    "sample_data = {\n",
    "    \"file_name\": \"research_data.csv\",\n",
    "    \"content\": \"\"\"id,name,measurement,timestamp\n",
    "1,Sample A,23.5,2024-01-01T10:00:00Z\n",
    "2,Sample B,45.2,2024-01-01T11:00:00Z\n",
    "3,Sample C,67.8,2024-01-01T12:00:00Z\n",
    "4,Sample D,12.3,2024-01-01T13:00:00Z\n",
    "5,Sample E,89.1,2024-01-01T14:00:00Z\"\"\",\n",
    "    \"description\": \"Research dataset with experimental measurements\",\n",
    "    \"content_type\": \"text/csv\",\n",
    "    \"tags\": [\"research\", \"measurements\", \"csv\", \"experiment\"]\n",
    "}\n",
    "\n",
    "print(f\"📄 Sample data prepared:\")\n",
    "print(f\"   File: {sample_data['file_name']}\")\n",
    "print(f\"   Description: {sample_data['description']}\")\n",
    "print(f\"   Type: {sample_data['content_type']}\")\n",
    "print(f\"   Size: {len(sample_data['content'].encode('utf-8'))} bytes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload file to S3 with metadata extraction\\n\n",
    "print(f\\\"📤 Uploading file to S3...\\\")\\n\\n\n",
    "file_data = sample_data['content'].encode('utf-8')\\n\n",
    "object_key = f\\\"data/{sample_data['file_name']}\\\"\\n\\n\n",
    "# Extract comprehensive metadata\\n\n",
    "file_hash = hashlib.sha256(file_data).hexdigest()\\n\n",
    "file_size = len(file_data)\\n\n",
    "created_at = datetime.now(timezone.utc).isoformat()\\n\\n\n",
    "# CSV-specific metadata\\n\n",
    "csv_lines = sample_data['content'].split('\\\\n')\\n\n",
    "csv_rows = len(csv_lines) - 1  # Exclude header\\n\n",
    "csv_columns = len(csv_lines[0].split(',')) if csv_lines else 0\\n\\n\n",
    "metadata = {\\n\n",
    "    \\\"file_name\\\": sample_data['file_name'],\\n\n",
    "    \\\"file_size\\\": file_size,\\n\n",
    "    \\\"content_type\\\": sample_data['content_type'],\\n\n",
    "    \\\"sha256_hash\\\": file_hash,\\n\n",
    "    \\\"description\\\": sample_data['description'],\\n\n",
    "    \\\"tags\\\": sample_data['tags'],\\n\n",
    "    \\\"created_at\\\": created_at,\\n\n",
    "    \\\"format\\\": \\\"CSV\\\",\\n\n",
    "    \\\"csv_rows\\\": csv_rows,\\n\n",
    "    \\\"csv_columns\\\": csv_columns,\\n\n",
    "    \\\"workflow_id\\\": f\\\"{WORKFLOW_PREFIX}_{TIMESTAMP}\\\"\\n\n",
    "}\\n\\n\n",
    "try:\\n\n",
    "    upload_result = client.upload_object(\\n\n",
    "        bucket_name=WORKFLOW_BUCKET,\\n\n",
    "        object_key=object_key,\\n\n",
    "        file_data=file_data,\\n\n",
    "        content_type=sample_data['content_type']\\n\n",
    "    )\\n\n",
    "    \\n\n",
    "    print(f\\\"✅ File uploaded successfully\\\")\\n\n",
    "    print(f\\\"📍 S3 URI: s3://{WORKFLOW_BUCKET}/{object_key}\\\")\\n\n",
    "    print(f\\\"📊 Size: {file_size} bytes\\\")\\n\n",
    "    print(f\\\"🔒 SHA-256: {file_hash[:16]}...\\\")\\n\n",
    "    print(f\\\"📋 Rows: {csv_rows}, Columns: {csv_columns}\\\")\\n\n",
    "    \\n\n",
    "except Exception as e:\\n\n",
    "    print(f\\\"❌ Upload failed: {e}\\\")\\n\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 🔗 Step 2: Generate Secure Access URL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate presigned download URL\\n\n",
    "print(f\\\"🔗 Generating secure access URL...\\\")\\n\\n\n",
    "URL_EXPIRATION = 604800  # 7 days\\n\\n\n",
    "try:\\n\n",
    "    url_data = client.generate_presigned_download_url(\\n\n",
    "        bucket_name=WORKFLOW_BUCKET,\\n\n",
    "        object_key=object_key,\\n\n",
    "        expiration=URL_EXPIRATION\\n\n",
    "    )\\n\n",
    "    \\n\n",
    "    download_url = url_data.get('url', '')\\n\n",
    "    url_generated_at = datetime.now(timezone.utc).isoformat()\\n\n",
    "    \\n\n",
    "    print(f\\\"✅ Presigned URL generated successfully\\\")\\n\n",
    "    print(f\\\"🔗 URL: {download_url[:60]}...\\\")\\n\n",
    "    print(f\\\"⏰ Expires in: {URL_EXPIRATION // 86400} days\\\")\\n\n",
    "    print(f\\\"📅 Generated at: {url_generated_at[:19]}Z\\\")\\n\n",
    "    \\n\n",
    "    # Add URL info to metadata\\n\n",
    "    metadata.update({\\n\n",
    "        \\\"download_url\\\": download_url,\\n\n",
    "        \\\"url_expires_days\\\": URL_EXPIRATION // 86400,\\n\n",
    "        \\\"url_generated_at\\\": url_generated_at\\n\n",
    "    })\\n\n",
    "    \\n\n",
    "except Exception as e:\\n\n",
    "    print(f\\\"❌ URL generation failed: {e}\\\")\\n\n",
    "    download_url = \\\"\\\"\\n\n",
    "    # Continue without URL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. 📋 Step 3: Register Dataset with S3 References"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comprehensive dataset with S3 metadata\n",
    "print(f\"📋 Registering dataset with S3 references...\")\n",
    "\n",
    "dataset_name = f\"{WORKFLOW_PREFIX}_dataset_{TIMESTAMP}\"\n",
    "dataset_title = f\"{metadata['description']} - {metadata['file_name']}\"\n",
    "\n",
    "# Create rich dataset description\n",
    "dataset_description = f\"\"\"Data Integration Workflow Dataset\n",
    "\n",
    "{metadata['description']}\n",
    "\n",
    "File Information:\n",
    "- Original file: {metadata['file_name']}\n",
    "- Size: {metadata['file_size']} bytes\n",
    "- Format: {metadata['format']}\n",
    "- Content type: {metadata['content_type']}\n",
    "- SHA-256 hash: {metadata['sha256_hash']}\n",
    "\n",
    "S3 Storage:\n",
    "- Bucket: {WORKFLOW_BUCKET}\n",
    "- Object key: {object_key}\n",
    "- S3 URI: s3://{WORKFLOW_BUCKET}/{object_key}\n",
    "\n",
    "CSV Details:\n",
    "- Data rows: {metadata['csv_rows']}\n",
    "- Columns: {metadata['csv_columns']}\n",
    "- Header: id, name, measurement, timestamp\n",
    "\n",
    "Access Information:\n",
    "- Download URL expires in {metadata.get('url_expires_days', 'N/A')} days\n",
    "- Created: {metadata['created_at'][:19]}Z\n",
    "- Workflow ID: {metadata['workflow_id']}\"\"\"\n",
    "\n",
    "print(f\"📝 Dataset name: {dataset_name}\")\n",
    "print(f\"📖 Title: {dataset_title}\")\n",
    "print(f\"🏢 Organization: {DATASET_ORG}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Register dataset with comprehensive metadata\\n\n",
    "dataset_data = {\\n\n",
    "    \\\"name\\\": dataset_name,\\n\n",
    "    \\\"title\\\": dataset_title,\\n\n",
    "    \\\"owner_org\\\": DATASET_ORG,\\n\n",
    "    \\\"notes\\\": dataset_description,\\n\n",
    "    \\\"tags\\\": metadata['tags'],\\n\n",
    "    \\\"license_id\\\": \\\"cc-by\\\",\\n\n",
    "    \\\"private\\\": False,\\n\n",
    "    \\\"extras\\\": {\\n\n",
    "        # File metadata\\n\n",
    "        \\\"file_name\\\": metadata['file_name'],\\n\n",
    "        \\\"file_size\\\": str(metadata['file_size']),\\n\n",
    "        \\\"content_type\\\": metadata['content_type'],\\n\n",
    "        \\\"sha256_hash\\\": metadata['sha256_hash'],\\n\n",
    "        \\\"format\\\": metadata['format'],\\n\n",
    "        \\n\n",
    "        # S3 location\\n\n",
    "        \\\"s3_bucket\\\": WORKFLOW_BUCKET,\\n\n",
    "        \\\"s3_object_key\\\": object_key,\\n\n",
    "        \\\"s3_uri\\\": f\\\"s3://{WORKFLOW_BUCKET}/{object_key}\\\",\\n\n",
    "        \\n\n",
    "        # CSV-specific metadata\\n\n",
    "        \\\"csv_rows\\\": str(metadata['csv_rows']),\\n\n",
    "        \\\"csv_columns\\\": str(metadata['csv_columns']),\\n\n",
    "        \\n\n",
    "        # Workflow tracking\\n\n",
    "        \\\"workflow_id\\\": metadata['workflow_id'],\\n\n",
    "        \\\"created_at\\\": metadata['created_at']\\n\n",
    "    }\\n\n",
    "}\\n\\n\n",
    "# Add URL information if available\\n\n",
    "if download_url:\\n\n",
    "    dataset_data['extras'].update({\\n\n",
    "        \\\"download_url\\\": metadata['download_url'],\\n\n",
    "        \\\"url_expires_days\\\": str(metadata['url_expires_days']),\\n\n",
    "        \\\"url_generated_at\\\": metadata['url_generated_at']\\n\n",
    "    })\\n\\n\n",
    "try:\\n\n",
    "    registration_result = client.register_general_dataset(dataset_data, server=\\\"local\\\")\\n\n",
    "    dataset_id = registration_result.get('id', 'unknown')\\n\n",
    "    \\n\n",
    "    print(f\\\"✅ Dataset registered successfully\\\")\\n\n",
    "    print(f\\\"🆔 Dataset ID: {dataset_id}\\\")\\n\n",
    "    print(f\\\"📊 Metadata fields: {len(dataset_data['extras'])}\\\")\\n\n",
    "    print(f\\\"🔗 S3 integration: Complete\\\")\\n\n",
    "    print(f\\\"🏷️  Tags: {', '.join(metadata['tags'])}\\\")\\n\n",
    "    \\n\n",
    "except Exception as e:\\n\n",
    "    print(f\\\"❌ Dataset registration failed: {e}\\\")\\n\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. 🔍 Step 4: Verify Integration and Discovery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify dataset through search\\n\n",
    "print(f\\\"🔍 Verifying dataset integration...\\\")\\n\\n\n",
    "try:\\n\n",
    "    # Search by workflow prefix\\n\n",
    "    search_results = client.search_datasets(\\n\n",
    "        terms=[WORKFLOW_PREFIX],\\n\n",
    "        server=\\\"local\\\"\\n\n",
    "    )\\n\n",
    "    \\n\n",
    "    workflow_datasets = [\\n\n",
    "        ds for ds in search_results \\n\n",
    "        if ds.get('name', '').startswith(WORKFLOW_PREFIX)\\n\n",
    "    ]\\n\n",
    "    \\n\n",
    "    print(f\\\"✅ Search verification successful\\\")\\n\n",
    "    print(f\\\"📊 Found {len(workflow_datasets)} workflow datasets\\\")\\n\n",
    "    \\n\n",
    "    if workflow_datasets:\\n\n",
    "        dataset = workflow_datasets[0]\\n\n",
    "        print(f\\\"\\n📋 Dataset Details:\\\")\\n\n",
    "        print(f\\\"   Name: {dataset.get('name', 'Unknown')}\\\")\\n\n",
    "        print(f\\\"   Title: {dataset.get('title', 'No title')}\\\")\\n\n",
    "        print(f\\\"   Organization: {dataset.get('organization', {}).get('name', 'Unknown')}\\\")\\n\n",
    "        \\n\n",
    "        # Show tags\\n\n",
    "        tags = dataset.get('tags', [])\\n\n",
    "        if tags:\\n\n",
    "            tag_names = [tag.get('name', tag) if isinstance(tag, dict) else str(tag) for tag in tags]\\n\n",
    "            print(f\\\"   Tags: {', '.join(tag_names)}\\\")\\n\n",
    "    \\n\n",
    "except Exception as e:\\n\n",
    "    print(f\\\"❌ Search verification failed: {e}\\\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Advanced search demonstrations\n",
    "print(f\"\\n🔎 Advanced Search Demonstrations\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "search_queries = [\n",
    "    {\"name\": \"By tag\", \"terms\": [\"research\"]},\n",
    "    {\"name\": \"By format\", \"terms\": [\"csv\"]},\n",
    "    {\"name\": \"By organization\", \"terms\": [DATASET_ORG]}\n",
    "]\n",
    "\n",
    "for query in search_queries:\n",
    "    try:\n",
    "        results = client.search_datasets(terms=query[\"terms\"], server=\"local\")\n",
    "        workflow_matches = [r for r in results if r.get('name', '').startswith(WORKFLOW_PREFIX)]\n",
    "        \n",
    "        print(f\"🔍 {query['name']}: {len(results)} total, {len(workflow_matches)} workflow matches\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ {query['name']} search failed: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. 🧹 Resource Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleanup confirmation\\n\n",
    "print(\\\"🧹 CLEANUP CONFIRMATION\\\")\\n\n",
    "print(\\\"=\\\" * 30)\\n\n",
    "print(f\\\"Resources to delete:\\\")\\n\n",
    "print(f\\\"   📋 Dataset: {dataset_name}\\\")\\n\n",
    "print(f\\\"   📁 S3 object: {object_key}\\\")\\n\n",
    "print(f\\\"   🪣 S3 bucket: {WORKFLOW_BUCKET}\\\")\\n\\n\n",
    "confirmation = input(\\\"Proceed with cleanup? (yes/no): \\\").strip().lower()\\n\\n\n",
    "if confirmation != 'yes':\\n\n",
    "    print(\\\"🚫 Cleanup cancelled - resources remain for exploration\\\")\\n\n",
    "else:\\n\n",
    "    print(\\\"✅ Proceeding with cleanup...\\\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute cleanup if confirmed\n",
    "if confirmation == 'yes':\n",
    "    print(f\"\\n🗑️  Cleaning up resources...\")\n",
    "    \n",
    "    # Delete dataset\n",
    "    try:\n",
    "        client.delete_resource_by_name(dataset_name, server=\"local\")\n",
    "        print(f\"✅ Dataset deleted: {dataset_name}\")\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Dataset deletion failed: {e}\")\n",
    "    \n",
    "    # Delete S3 object\n",
    "    try:\n",
    "        client.delete_object(WORKFLOW_BUCKET, object_key)\n",
    "        print(f\"✅ S3 object deleted: {object_key}\")\n",
    "    except Exception as e:\n",
    "        print(f\"❌ S3 object deletion failed: {e}\")\n",
    "    \n",
    "    # Delete S3 bucket\n",
    "    try:\n",
    "        client.delete_bucket(WORKFLOW_BUCKET)\n",
    "        print(f\"✅ S3 bucket deleted: {WORKFLOW_BUCKET}\")\n",
    "    except Exception as e:\n",
    "        print(f\"❌ S3 bucket deletion failed: {e}\")\n",
    "    \n",
    "    print(f\"\\n🎉 Cleanup completed successfully!\")\n",
    "else:\n",
    "    print(f\"\\n💡 Resources remain available for further exploration:\")\n",
    "    print(f\"   🪣 S3 bucket: {WORKFLOW_BUCKET}\")\n",
    "    print(f\"   📁 S3 object: s3://{WORKFLOW_BUCKET}/{object_key}\")\n",
    "    print(f\"   📋 Dataset: {dataset_name} (ID: {dataset_id})\")\n",
    "    if download_url:\n",
    "        print(f\"   🔗 Download URL: Available for {metadata.get('url_expires_days', 'N/A')} days\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
